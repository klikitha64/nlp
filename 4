from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
import numpy as np

# Example data
documents = ["This is the first document.",
             "This document is the second document.",
             "And this is the third one.",
             "Is this the first document?"]

# Create CountVectorizer object
vectorizer = CountVectorizer()

# Fit the vectorizer to the data
X = vectorizer.fit_transform(documents)

# Instantiate and fit TruncatedSVD
lsa = TruncatedSVD(n_components=2)
lsa.fit(X)

# Get feature names
terms = vectorizer.get_feature_names_out()


topic_matrix = np.array([lsa.components_[i] / np.linalg.norm(lsa.components_[i]) for i in
range(lsa.components_.shape[0])])
print("Top terms for each topic:")
for i, topic in enumerate(topic_matrix):
    top_indices = topic.argsort()[-5:][::-1] 
    top_terms = [terms[index] for index in top_indices]
    print(f"Topic {i + 1}: {' '.join(top_terms)}")
